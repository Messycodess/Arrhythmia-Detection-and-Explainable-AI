{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Messycodess/Arrhythmia-Detection-and-Explainable-AI/blob/main/Models11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRVHwhfd7yY2",
        "outputId": "3f64e776-06b0-4fa1-b7e1-aac0f8d1a57f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Mounted at /content/drive\n",
            "Data path: /content/drive/MyDrive/ECG_Datasets/MIT-BIH\n",
            "Results path: /content/drive/MyDrive/ECG_Datasets/Results\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m139.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mTotal records found: 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading records: 100%|██████████| 48/48 [00:10<00:00,  4.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Signals shape: (112619, 360) Labels shape: (112619,)\n",
            "Train/Val/Test sizes: 81085 9010 22524\n",
            "\n",
            "============================================================\n",
            "Training model: CNN1D\n",
            "============================================================\n",
            "[CNN1D] Epoch 1/20 - Train Loss: 0.2865 Acc: 90.01% | Val Loss: 0.1515 Acc: 95.53%\n",
            "[CNN1D] Epoch 2/20 - Train Loss: 0.1483 Acc: 94.81% | Val Loss: 0.1622 Acc: 94.88%\n",
            "[CNN1D] Epoch 3/20 - Train Loss: 0.1109 Acc: 96.04% | Val Loss: 0.1385 Acc: 95.74%\n",
            "[CNN1D] Epoch 4/20 - Train Loss: 0.0879 Acc: 97.01% | Val Loss: 0.0876 Acc: 97.58%\n",
            "[CNN1D] Epoch 5/20 - Train Loss: 0.0782 Acc: 97.30% | Val Loss: 0.0748 Acc: 97.85%\n",
            "[CNN1D] Epoch 6/20 - Train Loss: 0.0643 Acc: 97.77% | Val Loss: 0.0823 Acc: 97.86%\n",
            "[CNN1D] Epoch 7/20 - Train Loss: 0.0563 Acc: 98.05% | Val Loss: 0.0804 Acc: 97.86%\n",
            "[CNN1D] Epoch 8/20 - Train Loss: 0.0501 Acc: 98.33% | Val Loss: 0.0767 Acc: 98.15%\n",
            "[CNN1D] Epoch 9/20 - Train Loss: 0.0463 Acc: 98.48% | Val Loss: 0.0698 Acc: 98.46%\n",
            "[CNN1D] Epoch 10/20 - Train Loss: 0.0434 Acc: 98.57% | Val Loss: 0.0698 Acc: 98.48%\n",
            "[CNN1D] Epoch 11/20 - Train Loss: 0.0395 Acc: 98.70% | Val Loss: 0.0709 Acc: 98.42%\n",
            "[CNN1D] Epoch 12/20 - Train Loss: 0.0399 Acc: 98.76% | Val Loss: 0.0589 Acc: 98.75%\n",
            "[CNN1D] Epoch 13/20 - Train Loss: 0.0341 Acc: 98.86% | Val Loss: 0.0632 Acc: 98.82%\n",
            "[CNN1D] Epoch 14/20 - Train Loss: 0.0320 Acc: 98.89% | Val Loss: 0.0655 Acc: 98.82%\n",
            "[CNN1D] Epoch 15/20 - Train Loss: 0.0280 Acc: 99.08% | Val Loss: 0.0658 Acc: 98.66%\n",
            "[CNN1D] Epoch 16/20 - Train Loss: 0.0310 Acc: 98.97% | Val Loss: 0.0583 Acc: 98.83%\n",
            "[CNN1D] Epoch 17/20 - Train Loss: 0.0262 Acc: 99.16% | Val Loss: 0.0769 Acc: 98.70%\n",
            "[CNN1D] Epoch 18/20 - Train Loss: 0.0276 Acc: 99.11% | Val Loss: 0.0646 Acc: 98.83%\n",
            "[CNN1D] Epoch 19/20 - Train Loss: 0.0254 Acc: 99.17% | Val Loss: 0.0600 Acc: 98.99%\n",
            "[CNN1D] Epoch 20/20 - Train Loss: 0.0231 Acc: 99.24% | Val Loss: 0.0693 Acc: 98.67%\n",
            "=== CNN1D Classification Report ===\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           N     0.9966    0.9866    0.9916     18121\n",
            "           S     0.8044    0.9245    0.8603       556\n",
            "           V     0.9454    0.9820    0.9634      1447\n",
            "           F     0.7447    0.8696    0.8023       161\n",
            "           Q     0.9854    0.9920    0.9886      2239\n",
            "\n",
            "    accuracy                         0.9845     22524\n",
            "   macro avg     0.8953    0.9509    0.9212     22524\n",
            "weighted avg     0.9857    0.9845    0.9849     22524\n",
            "\n",
            "\n",
            "============================================================\n",
            "Training model: ResNet18_1D\n",
            "============================================================\n",
            "[ResNet18_1D] Epoch 1/20 - Train Loss: 0.1855 Acc: 93.79% | Val Loss: 0.0996 Acc: 96.73%\n",
            "[ResNet18_1D] Epoch 2/20 - Train Loss: 0.0731 Acc: 97.60% | Val Loss: 0.0816 Acc: 97.47%\n",
            "[ResNet18_1D] Epoch 3/20 - Train Loss: 0.0470 Acc: 98.49% | Val Loss: 0.0867 Acc: 97.31%\n",
            "[ResNet18_1D] Epoch 4/20 - Train Loss: 0.0391 Acc: 98.74% | Val Loss: 0.0693 Acc: 98.09%\n",
            "[ResNet18_1D] Epoch 5/20 - Train Loss: 0.0326 Acc: 98.99% | Val Loss: 0.0482 Acc: 98.66%\n",
            "[ResNet18_1D] Epoch 6/20 - Train Loss: 0.0266 Acc: 99.18% | Val Loss: 0.0461 Acc: 98.83%\n",
            "[ResNet18_1D] Epoch 7/20 - Train Loss: 0.0238 Acc: 99.28% | Val Loss: 0.0478 Acc: 98.76%\n",
            "[ResNet18_1D] Epoch 8/20 - Train Loss: 0.0194 Acc: 99.42% | Val Loss: 0.0482 Acc: 98.76%\n",
            "[ResNet18_1D] Epoch 9/20 - Train Loss: 0.0175 Acc: 99.47% | Val Loss: 0.0641 Acc: 98.42%\n",
            "[ResNet18_1D] Epoch 10/20 - Train Loss: 0.0164 Acc: 99.50% | Val Loss: 0.0498 Acc: 98.81%\n",
            "[ResNet18_1D] Epoch 11/20 - Train Loss: 0.0135 Acc: 99.55% | Val Loss: 0.0463 Acc: 99.08%\n",
            "[ResNet18_1D] Epoch 12/20 - Train Loss: 0.0113 Acc: 99.67% | Val Loss: 0.0512 Acc: 98.92%\n",
            "[ResNet18_1D] Epoch 13/20 - Train Loss: 0.0108 Acc: 99.67% | Val Loss: 0.0503 Acc: 98.83%\n",
            "[ResNet18_1D] Epoch 14/20 - Train Loss: 0.0104 Acc: 99.71% | Val Loss: 0.0514 Acc: 98.80%\n",
            "[ResNet18_1D] Epoch 15/20 - Train Loss: 0.0101 Acc: 99.67% | Val Loss: 0.0370 Acc: 99.21%\n",
            "[ResNet18_1D] Epoch 16/20 - Train Loss: 0.0085 Acc: 99.75% | Val Loss: 0.0473 Acc: 98.98%\n",
            "[ResNet18_1D] Epoch 17/20 - Train Loss: 0.0081 Acc: 99.76% | Val Loss: 0.0423 Acc: 99.19%\n",
            "[ResNet18_1D] Epoch 18/20 - Train Loss: 0.0087 Acc: 99.76% | Val Loss: 0.0391 Acc: 99.25%\n",
            "[ResNet18_1D] Epoch 19/20 - Train Loss: 0.0063 Acc: 99.83% | Val Loss: 0.0456 Acc: 99.09%\n",
            "[ResNet18_1D] Epoch 20/20 - Train Loss: 0.0072 Acc: 99.77% | Val Loss: 0.0426 Acc: 99.25%\n",
            "=== ResNet18_1D Classification Report ===\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           N     0.9967    0.9950    0.9959     18121\n",
            "           S     0.9204    0.9353    0.9277       556\n",
            "           V     0.9716    0.9703    0.9710      1447\n",
            "           F     0.8383    0.8696    0.8537       161\n",
            "           Q     0.9841    0.9924    0.9882      2239\n",
            "\n",
            "    accuracy                         0.9908     22524\n",
            "   macro avg     0.9422    0.9525    0.9473     22524\n",
            "weighted avg     0.9908    0.9908    0.9908     22524\n",
            "\n",
            "\n",
            "============================================================\n",
            "Training model: InceptionTime\n",
            "============================================================\n",
            "[InceptionTime] Epoch 1/20 - Train Loss: 0.4954 Acc: 83.52% | Val Loss: 0.1732 Acc: 95.49%\n",
            "[InceptionTime] Epoch 2/20 - Train Loss: 0.2404 Acc: 92.34% | Val Loss: 0.4294 Acc: 86.99%\n",
            "[InceptionTime] Epoch 3/20 - Train Loss: 0.1610 Acc: 94.97% | Val Loss: 0.5737 Acc: 79.90%\n",
            "[InceptionTime] Epoch 4/20 - Train Loss: 0.1280 Acc: 95.97% | Val Loss: 0.5510 Acc: 77.24%\n",
            "[InceptionTime] Epoch 5/20 - Train Loss: 0.1066 Acc: 96.66% | Val Loss: 0.4991 Acc: 81.88%\n"
          ]
        }
      ],
      "source": [
        "# 4 Models (Simple CNN, Resnet, Inception Time, Xception )\n",
        "# MIT-BIH dataset .dat/.hea/.atr files are in:\n",
        "# /content/drive/MyDrive/ECG_Datasets/MIT-BIH\n",
        "# Results saved to:\n",
        "# /content/drive/MyDrive/ECG_Datasets/Results/<model_name>/\n",
        "\n",
        "# ===============================\n",
        "# STEP 0: GPU & Misc\n",
        "# ===============================\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# reproducibility (best effort)\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ===============================\n",
        "# STEP 1: Mount Google Drive\n",
        "# ===============================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "DATA_ROOT = \"/content/drive/MyDrive/ECG_Datasets\"\n",
        "MITBIH_PATH = os.path.join(DATA_ROOT, \"MIT-BIH\")\n",
        "RESULTS_ROOT = os.path.join(DATA_ROOT, \"Results\")\n",
        "os.makedirs(RESULTS_ROOT, exist_ok=True)\n",
        "print(\"Data path:\", MITBIH_PATH)\n",
        "print(\"Results path:\", RESULTS_ROOT)\n",
        "\n",
        "# ===============================\n",
        "# STEP 2: Install & Import Libraries\n",
        "# ===============================\n",
        "!pip install -q wfdb\n",
        "import wfdb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "\n",
        "# ===============================\n",
        "# STEP 3: AAMI mapping (5 classes)\n",
        "# ===============================\n",
        "AAMI_CLASSES = {\n",
        "    'N': ['N','L','R','e','j'],      # Normal\n",
        "    'S': ['A','a','J','S'],          # Supraventricular ectopic\n",
        "    'V': ['V','E'],                  # Ventricular ectopic\n",
        "    'F': ['F'],                      # Fusion\n",
        "    'Q': ['/', 'f','Q']              # Unknown / paced\n",
        "}\n",
        "classes = list(AAMI_CLASSES.keys())\n",
        "\n",
        "symbol_to_class = {}\n",
        "for idx, key in enumerate(AAMI_CLASSES.keys()):\n",
        "    for sym in AAMI_CLASSES[key]:\n",
        "        symbol_to_class[sym] = idx\n",
        "\n",
        "# ===============================\n",
        "# STEP 4: Load & Preprocess MIT-BIH\n",
        "# ===============================\n",
        "records = [f.split('.')[0] for f in os.listdir(MITBIH_PATH) if f.endswith('.dat')]\n",
        "records = sorted(records)\n",
        "print(\"Total records found:\", len(records))\n",
        "\n",
        "signals = []\n",
        "labels = []\n",
        "win_size = 360  # 1 second at 360Hz\n",
        "\n",
        "for rec in tqdm(records, desc=\"Reading records\"):\n",
        "    rec_path = os.path.join(MITBIH_PATH, rec)\n",
        "    try:\n",
        "        record = wfdb.rdrecord(rec_path)\n",
        "        annotation = wfdb.rdann(rec_path, 'atr')\n",
        "    except Exception as e:\n",
        "        print(\"Failed to read\", rec, e)\n",
        "        continue\n",
        "\n",
        "    # use first channel (Lead II) if available\n",
        "    signal = record.p_signal[:, 0]\n",
        "    # z-score normalize per record\n",
        "    signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-9)\n",
        "\n",
        "    for idx, beat_sample in enumerate(annotation.sample):\n",
        "        start = int(max(0, beat_sample - win_size // 2))\n",
        "        end = start + win_size\n",
        "        if end > len(signal):\n",
        "            continue\n",
        "        seg = signal[start:end]\n",
        "        signals.append(seg)\n",
        "\n",
        "        sym = annotation.symbol[idx]\n",
        "        class_idx = symbol_to_class.get(sym, len(AAMI_CLASSES)-1)  # default Q\n",
        "        labels.append(class_idx)\n",
        "\n",
        "signals = np.array(signals, dtype=np.float32)\n",
        "labels = np.array(labels, dtype=np.int64)\n",
        "print(\"Signals shape:\", signals.shape, \"Labels shape:\", labels.shape)\n",
        "\n",
        "# ===============================\n",
        "# STEP 5: Train/Val/Test split (we'll do train/test; can split val from train)\n",
        "# ===============================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    signals, labels, test_size=0.2, random_state=seed, stratify=labels\n",
        ")\n",
        "\n",
        "# Use a small validation split from train for monitoring (10% of train)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.1, random_state=seed, stratify=y_train\n",
        ")\n",
        "\n",
        "print(\"Train/Val/Test sizes:\", X_train.shape[0], X_val.shape[0], X_test.shape[0])\n",
        "\n",
        "# ===============================\n",
        "# STEP 6: PyTorch Dataset & DataLoader with Weighted Sampler\n",
        "# ===============================\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)  # (N,1,L)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = ECGDataset(X_train, y_train)\n",
        "val_dataset = ECGDataset(X_val, y_val)\n",
        "test_dataset = ECGDataset(X_test, y_test)\n",
        "\n",
        "# Weighted sampler to address imbalance\n",
        "class_sample_counts = np.bincount(y_train)\n",
        "class_weights = 1. / (class_sample_counts + 1e-9)\n",
        "samples_weights = class_weights[y_train]\n",
        "sampler = WeightedRandomSampler(weights=samples_weights, num_samples=len(samples_weights), replacement=True)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, drop_last=False, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "# ===============================\n",
        "# STEP 7: Define Models\n",
        "# - CNN1D (baseline kept)\n",
        "# - ResNet18-1D\n",
        "# - InceptionTime (standard)\n",
        "# - Mini Xception-1D (depthwise separable)\n",
        "# ===============================\n",
        "\n",
        "# ---- CNN1D (baseline) ----\n",
        "class CNN1D(nn.Module):\n",
        "    def __init__(self, input_length, num_classes=5):\n",
        "        super(CNN1D, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "\n",
        "        # compute linear size\n",
        "        x = torch.randn(1,1,input_length)\n",
        "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
        "        flat = x.numel()\n",
        "        self.fc1 = nn.Linear(flat, 128)\n",
        "        self.drop = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# ---- ResNet1D (ResNet18-like) ----\n",
        "def conv1d_bn_relu(in_ch, out_ch, kernel_size, stride=1, padding=0):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv1d(in_ch, out_ch, kernel_size, stride=stride, padding=padding, bias=False),\n",
        "        nn.BatchNorm1d(out_ch),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "class BasicBlock1D(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock1D, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(planes)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet1D(nn.Module):\n",
        "    def __init__(self, block, layers, input_length, num_classes=5):\n",
        "        super(ResNet1D, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # layers\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        # global pooling -> fc\n",
        "        # compute flatten size dynamically\n",
        "        x = torch.randn(1,1,input_length)\n",
        "        x = self.maxpool(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x); x = self.layer4(x)\n",
        "        x = nn.functional.adaptive_avg_pool1d(x, 1)\n",
        "        flat = x.view(x.size(0), -1).shape[1]\n",
        "        self.fc = nn.Linear(flat, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv1d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(planes * block.expansion),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.maxpool(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x); x = self.layer4(x)\n",
        "        x = nn.functional.adaptive_avg_pool1d(x, 1)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def ResNet18_1D(input_length, num_classes=5):\n",
        "    return ResNet1D(BasicBlock1D, [2,2,2,2], input_length, num_classes)\n",
        "\n",
        "# ---- InceptionTime building blocks (standard InceptionTime) ----\n",
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(InceptionModule, self).__init__()\n",
        "        # following InceptionTime: bottleneck + conv kernels 39, 19, 9 (common choices)\n",
        "        bottleneck_channels = max(1, in_channels // 4)\n",
        "        self.bottleneck = nn.Conv1d(in_channels, bottleneck_channels, kernel_size=1, bias=False)\n",
        "        self.conv1 = nn.Conv1d(bottleneck_channels, out_channels, kernel_size=39, padding=19, bias=False)\n",
        "        self.conv2 = nn.Conv1d(bottleneck_channels, out_channels, kernel_size=19, padding=9, bias=False)\n",
        "        self.conv3 = nn.Conv1d(bottleneck_channels, out_channels, kernel_size=9, padding=4, bias=False)\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        )\n",
        "        self.bn = nn.BatchNorm1d(out_channels * 4)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.shape[1] == 1:\n",
        "            bx = self.bottleneck(x)\n",
        "        else:\n",
        "            bx = self.bottleneck(x)\n",
        "        y1 = self.conv1(bx)\n",
        "        y2 = self.conv2(bx)\n",
        "        y3 = self.conv3(bx)\n",
        "        y4 = self.maxpool_conv(x)\n",
        "        y = torch.cat([y1, y2, y3, y4], dim=1)\n",
        "        y = self.bn(y)\n",
        "        y = self.relu(y)\n",
        "        return y\n",
        "\n",
        "class InceptionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(InceptionBlock, self).__init__()\n",
        "        self.incep = InceptionModule(in_channels, out_channels)\n",
        "        self.residual = None\n",
        "        if in_channels != out_channels*4:\n",
        "            self.residual = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels*4, kernel_size=1, bias=False),\n",
        "                nn.BatchNorm1d(out_channels*4)\n",
        "            )\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.incep(x)\n",
        "        if self.residual is not None:\n",
        "            x = self.residual(x)\n",
        "        return self.relu(x + y)\n",
        "\n",
        "class InceptionTime(nn.Module):\n",
        "    def __init__(self, input_length, num_classes=5, num_modules=6, in_channels=1, out_channels=32):\n",
        "        super(InceptionTime, self).__init__()\n",
        "        channels = in_channels\n",
        "        modules = []\n",
        "        for _ in range(num_modules):\n",
        "            modules.append(InceptionBlock(channels, out_channels))\n",
        "            channels = out_channels * 4\n",
        "        self.network = nn.Sequential(*modules)\n",
        "        # global pooling and fc\n",
        "        x = torch.randn(1,1,input_length)\n",
        "        x = self.network(x)\n",
        "        x = nn.functional.adaptive_avg_pool1d(x, 1)\n",
        "        flat = x.view(x.size(0), -1).shape[1]\n",
        "        self.fc = nn.Linear(flat, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.network(x)\n",
        "        x = nn.functional.adaptive_avg_pool1d(x, 1)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# ---- Mini Xception 1D (depthwise separable) ----\n",
        "class SeparableConv1d(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, padding=1):\n",
        "        super(SeparableConv1d, self).__init__()\n",
        "        self.depthwise = nn.Conv1d(in_ch, in_ch, kernel_size=kernel_size, padding=padding, groups=in_ch, bias=False)\n",
        "        self.pointwise = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n",
        "        self.bn = nn.BatchNorm1d(out_ch)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        x = self.bn(x)\n",
        "        return self.relu(x)\n",
        "\n",
        "class MiniXception1D(nn.Module):\n",
        "    def __init__(self, input_length, num_classes=5):\n",
        "        super(MiniXception1D, self).__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.sep1 = SeparableConv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "        self.sep2 = SeparableConv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "        self.sep3 = SeparableConv1d(128, 256, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.AdaptiveAvgPool1d(1)\n",
        "        x = torch.randn(1,1,input_length)\n",
        "        x = self.block1(x)\n",
        "        x = self.sep1(x); x = self.pool1(x)\n",
        "        x = self.sep2(x); x = self.pool2(x)\n",
        "        x = self.sep3(x); x = self.pool3(x)\n",
        "        flat = x.view(x.size(0), -1).shape[1]\n",
        "        self.fc = nn.Linear(flat, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.sep1(x); x = self.pool1(x)\n",
        "        x = self.sep2(x); x = self.pool2(x)\n",
        "        x = self.sep3(x); x = self.pool3(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# ===============================\n",
        "# STEP 8: Utilities (train/validate/evaluate + save)\n",
        "# ===============================\n",
        "import time\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "def train_one_model(model, model_name, num_epochs=20, lr=1e-3):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_correct = 0\n",
        "        running_total = 0\n",
        "        for X, y in train_loader:\n",
        "            X = X.to(device); y = y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X)\n",
        "            loss = criterion(outputs, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * X.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_correct += (preds == y).sum().item()\n",
        "            running_total += X.size(0)\n",
        "\n",
        "        train_loss = running_loss / running_total\n",
        "        train_acc = running_correct / running_total\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X = X.to(device); y = y.to(device)\n",
        "                outputs = model(X)\n",
        "                loss = criterion(outputs, y)\n",
        "                val_loss += loss.item() * X.size(0)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_correct += (preds == y).sum().item()\n",
        "                val_total += X.size(0)\n",
        "\n",
        "        val_loss = val_loss / val_total\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        print(f\"[{model_name}] Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} Acc: {train_acc*100:.2f}% | Val Loss: {val_loss:.4f} Acc: {val_acc*100:.2f}%\")\n",
        "\n",
        "        # save best\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_state = { \"model\": model.state_dict(), \"optimizer\": optimizer.state_dict(), \"epoch\": epoch }\n",
        "\n",
        "    # save final & best\n",
        "    model_dir = os.path.join(RESULTS_ROOT, model_name)\n",
        "    Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # final model\n",
        "    final_path = os.path.join(model_dir, f\"{model_name}_final.pt\")\n",
        "    torch.save(model.state_dict(), final_path)\n",
        "\n",
        "    # best checkpoint\n",
        "    if best_state is not None:\n",
        "        best_path = os.path.join(model_dir, f\"{model_name}_best.pt\")\n",
        "        torch.save(best_state[\"model\"], best_path)\n",
        "\n",
        "    # save history\n",
        "    hist_path = os.path.join(model_dir, f\"{model_name}_history.npy\")\n",
        "    np.save(hist_path, history)\n",
        "\n",
        "    return model, history, model_dir\n",
        "\n",
        "def evaluate_and_save(model, model_name, model_dir):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            X = X.to(device); y = y.to(device)\n",
        "            outputs = model(X)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel('Predicted'); plt.ylabel('True')\n",
        "    plt.title(f\"{model_name} Confusion Matrix\")\n",
        "    cm_path = os.path.join(model_dir, f\"{model_name}_confusion_matrix.png\")\n",
        "    plt.savefig(cm_path, bbox_inches='tight', dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    # classification report\n",
        "    report = classification_report(all_labels, all_preds, target_names=classes, digits=4)\n",
        "    print(f\"=== {model_name} Classification Report ===\\n\", report)\n",
        "    report_path = os.path.join(model_dir, f\"{model_name}_classification_report.txt\")\n",
        "    with open(report_path, \"w\") as f:\n",
        "        f.write(report)\n",
        "\n",
        "    # per-class accuracy\n",
        "    per_class_acc = cm.diagonal() / (cm.sum(axis=1) + 1e-9)\n",
        "    stats = {\n",
        "        \"confusion_matrix\": cm.tolist(),\n",
        "        \"per_class_accuracy\": per_class_acc.tolist()\n",
        "    }\n",
        "    stats_path = os.path.join(model_dir, f\"{model_name}_stats.json\")\n",
        "    with open(stats_path, \"w\") as f:\n",
        "        json.dump(stats, f, indent=2)\n",
        "\n",
        "    return all_labels, all_preds, cm, report\n",
        "\n",
        "def plot_history(history, model_name, model_dir):\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(history[\"train_acc\"], label=\"Train Acc\")\n",
        "    plt.plot(history[\"val_acc\"], label=\"Val Acc\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(f\"{model_name} Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(f\"{model_name} Loss\")\n",
        "    plt.legend()\n",
        "    plot_path = os.path.join(model_dir, f\"{model_name}_acc_loss.png\")\n",
        "    plt.savefig(plot_path, bbox_inches='tight', dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "# ===============================\n",
        "# STEP 9: Instantiate models and run training/eval for each\n",
        "# ===============================\n",
        "input_length = X_train.shape[1]\n",
        "models_to_run = {\n",
        "    \"CNN1D\": CNN1D(input_length=input_length, num_classes=len(classes)),\n",
        "    \"ResNet18_1D\": ResNet18_1D(input_length=input_length, num_classes=len(classes)),\n",
        "    \"InceptionTime\": InceptionTime(input_length=input_length, num_classes=len(classes), num_modules=6, out_channels=32),\n",
        "    \"MiniXception1D\": MiniXception1D(input_length=input_length, num_classes=len(classes))\n",
        "}\n",
        "\n",
        "# training parameters\n",
        "NUM_EPOCHS = 20\n",
        "LR = 1e-3\n",
        "\n",
        "# Run models sequentially (memory friendly)\n",
        "results_summary = {}\n",
        "for name, model in models_to_run.items():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Training model:\", name)\n",
        "    print(\"=\"*60)\n",
        "    model, history, model_dir = train_one_model(model, name, num_epochs=NUM_EPOCHS, lr=LR)\n",
        "    plot_history(history, name, model_dir)\n",
        "    all_labels, all_preds, cm, report = evaluate_and_save(model, name, model_dir)\n",
        "    # save preds & labels\n",
        "    np.save(os.path.join(model_dir, f\"{name}_y_true.npy\"), all_labels)\n",
        "    np.save(os.path.join(model_dir, f\"{name}_y_pred.npy\"), all_preds)\n",
        "    results_summary[name] = {\"model_dir\": model_dir, \"best_val_acc\": max(history[\"val_acc\"]) if len(history[\"val_acc\"])>0 else None}\n",
        "\n",
        "# Save a summary file\n",
        "with open(os.path.join(RESULTS_ROOT, \"results_summary.json\"), \"w\") as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print(\"All models finished. Results saved to:\", RESULTS_ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6wtC-Mjm82l",
        "outputId": "523a811e-46e7-4ee9-fb3f-09314fe61582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grad CAM\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G9SqYswKGmqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======= Fixed Grad-CAM (no backward-hook conflict) =======\n",
        "import torch, torch.nn.functional as F, numpy as np, matplotlib.pyplot as plt, random\n",
        "from pathlib import Path\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "\n",
        "MODEL_NAME = \"ResNet18_1D\"\n",
        "RESULTS_ROOT = \"/content/drive/MyDrive/ECG_Datasets/Results\"\n",
        "SAVE_DIR = Path(RESULTS_ROOT) / MODEL_NAME\n",
        "\n",
        "model = globals().get('model')\n",
        "test_dataset = globals().get('test_dataset')\n",
        "device = globals().get('device')\n",
        "classes = globals().get('classes', ['N','S','V','F','Q'])\n",
        "\n",
        "if model is None or test_dataset is None:\n",
        "    raise RuntimeError(\"Model or test_dataset missing. Run the restore cell first.\")\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# helper: find last Conv1d layer and its name\n",
        "def find_last_conv1d(mod):\n",
        "    last = None\n",
        "    last_name = None\n",
        "    for name, m in mod.named_modules():\n",
        "        if isinstance(m, torch.nn.Conv1d):\n",
        "            last = m\n",
        "            last_name = name\n",
        "    return last, last_name\n",
        "\n",
        "target_layer, layer_name = find_last_conv1d(model)\n",
        "if target_layer is None:\n",
        "    raise RuntimeError(\"No Conv1d layer found in model for Grad-CAM.\")\n",
        "\n",
        "print(\"Using target layer:\", layer_name)\n",
        "\n",
        "# Grad-CAM using torch.autograd.grad (no backward hooks)\n",
        "def gradcam_1d_nohook(model, input_tensor, target_class, target_layer):\n",
        "    \"\"\"\n",
        "    input_tensor: (1,1,L) tensor on device\n",
        "    returns: cam_norm (L,) and predicted probability for target_class\n",
        "    \"\"\"\n",
        "    activations = []\n",
        "\n",
        "    # forward hook to capture activations (DO NOT detach)\n",
        "    def forward_hook(m, inp, out):\n",
        "        activations.append(out)   # keep tensor requiring grad\n",
        "\n",
        "    fh = target_layer.register_forward_hook(forward_hook)\n",
        "\n",
        "    out = model(input_tensor)          # forward pass\n",
        "    score = out[0, target_class]       # scalar tensor\n",
        "\n",
        "    # compute gradients of score w.r.t. activations[0]\n",
        "    if len(activations) == 0:\n",
        "        fh.remove()\n",
        "        raise RuntimeError(\"Activation not captured by forward hook.\")\n",
        "    act = activations[0]               # shape (1, C, Lf), requires_grad=True\n",
        "    grads = torch.autograd.grad(outputs=score, inputs=act, retain_graph=False)[0]  # (1,C,Lf)\n",
        "\n",
        "    fh.remove()\n",
        "\n",
        "    # channel weights: global average pooling over time dim\n",
        "    weights = grads.mean(dim=2, keepdim=True)   # (1,C,1)\n",
        "    # weighted sum\n",
        "    cam = F.relu((weights * act).sum(dim=1, keepdim=True))  # (1,1,Lf)\n",
        "    # upsample to input length\n",
        "    cam_up = F.interpolate(cam, size=input_tensor.shape[-1], mode='linear', align_corners=False)\n",
        "    cam_np = cam_up.squeeze().detach().cpu().numpy()\n",
        "    cam_norm = (cam_np - cam_np.min()) / (cam_np.max() - cam_np.min() + 1e-9)\n",
        "\n",
        "    prob = float(torch.softmax(out.detach(), dim=1)[0, target_class].cpu().numpy())\n",
        "    return cam_norm, prob\n",
        "\n",
        "# pick representatives (light scan)\n",
        "MAX_SCAN = 2000\n",
        "n_test = len(test_dataset)\n",
        "idxs = list(range(n_test))\n",
        "if n_test > MAX_SCAN:\n",
        "    idxs = random.sample(idxs, MAX_SCAN)\n",
        "\n",
        "softmax = torch.nn.Softmax(dim=1)\n",
        "rep = {i: None for i in range(len(classes))}\n",
        "\n",
        "# prefer high-confidence correct predictions\n",
        "with torch.no_grad():\n",
        "    for i in idxs:\n",
        "        x, y = test_dataset[i]\n",
        "        inp = x.unsqueeze(0).to(device)\n",
        "        out = model(inp)\n",
        "        probs = softmax(out).cpu().numpy()[0]\n",
        "        pred = int(np.argmax(probs)); true = int(y.item())\n",
        "        if pred == true and rep[true] is None:\n",
        "            rep[true] = i\n",
        "        if all(v is not None for v in rep.values()):\n",
        "            break\n",
        "\n",
        "# fallback: any sample of that class\n",
        "if not all(v is not None for v in rep.values()):\n",
        "    for i in idxs:\n",
        "        _, y = test_dataset[i]\n",
        "        cls = int(y.item())\n",
        "        if rep[cls] is None:\n",
        "            rep[cls] = i\n",
        "        if all(v is not None for v in rep.values()):\n",
        "            break\n",
        "\n",
        "print(\"Representative examples (class:index):\", rep)\n",
        "\n",
        "# create combined figure\n",
        "fig, axes = plt.subplots(len(classes), 1, figsize=(10, 1.6*len(classes)))\n",
        "for cls_idx, cls_name in enumerate(classes):\n",
        "    idx = rep[cls_idx]\n",
        "    if idx is None:\n",
        "        axes[cls_idx].text(0.5, 0.5, f\"No sample for {cls_name}\", ha='center')\n",
        "        continue\n",
        "    sample, y = test_dataset[idx]\n",
        "    x = sample.squeeze().cpu().numpy()\n",
        "    inp = sample.unsqueeze(0).to(device)\n",
        "\n",
        "    # get model prediction\n",
        "    with torch.no_grad():\n",
        "        out = model(inp)\n",
        "        pred = int(out.argmax(1).cpu().numpy()[0])\n",
        "\n",
        "    # compute cam using gradcam function (no hooks conflict)\n",
        "    cam, prob = gradcam_1d_nohook(model, inp, pred, target_layer)\n",
        "\n",
        "    t = np.arange(len(x))\n",
        "    ax = axes[cls_idx]\n",
        "    ax.plot(t, x, color='k', linewidth=0.8)\n",
        "    amp = x.max() - x.min() if x.max() != x.min() else 1.0\n",
        "    base = x.min() - 0.12*amp\n",
        "    overlay = base + cam * (0.25*amp)\n",
        "    ax.fill_between(t, base, overlay, where=cam>0, color='orange', alpha=0.6)\n",
        "    ax.scatter(t, x, c=cam, cmap='jet', s=5)\n",
        "    ax.set_xlim(0, len(x))\n",
        "    ax.set_yticks([])\n",
        "    ax.set_title(f\"{cls_name} — true:{classes[int(y)]} pred:{classes[pred]} p={prob:.2f}\", fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "combined_path = SAVE_DIR / f\"{MODEL_NAME}_gradcam_combined.png\"\n",
        "plt.savefig(combined_path, dpi=200, bbox_inches='tight')\n",
        "plt.close(fig)\n",
        "\n",
        "# display and print\n",
        "display(Image.open(str(combined_path)))\n",
        "print(\"Saved combined:\", combined_path)\n",
        "print(\"\\nSlide text (copy-paste):\")\n",
        "print(\"- Orange/red overlay = model importance (higher = more important).\")\n",
        "print(\"- Model focuses on QRS region for Normal, and on widened QRS for Ventricular beats.\")\n",
        "print(\"- Grad-CAM confirms model uses medically meaningful ECG features.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "phkCYjCoEeXj",
        "outputId": "ccba4010-54ef-48b5-8cf4-f9c5ea165153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Model or test_dataset missing. Run the restore cell first.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3134545454.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model or test_dataset missing. Run the restore cell first.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Model or test_dataset missing. Run the restore cell first."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= GRAD-CAM FUNCTION (must run this first) =================\n",
        "\n",
        "def gradcam_1d_nohook(model, input_tensor, target_class, target_layer):\n",
        "    \"\"\"\n",
        "    input_tensor: (1,1,L) tensor on device\n",
        "    returns: cam_norm (L,) and predicted probability for target_class\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    import torch.nn.functional as F\n",
        "    import numpy as np\n",
        "\n",
        "    activations = []\n",
        "\n",
        "    # forward hook to capture activations\n",
        "    def forward_hook(m, inp, out):\n",
        "        activations.append(out)\n",
        "\n",
        "    fh = target_layer.register_forward_hook(forward_hook)\n",
        "\n",
        "    # forward pass\n",
        "    out = model(input_tensor)\n",
        "    score = out[0, target_class]\n",
        "\n",
        "    # gradients of score wrt activations\n",
        "    act = activations[0]\n",
        "    grads = torch.autograd.grad(outputs=score, inputs=act)[0]\n",
        "\n",
        "    fh.remove()\n",
        "\n",
        "    # compute weights\n",
        "    weights = grads.mean(dim=2, keepdim=True)  # (1,C,1)\n",
        "\n",
        "    # weighted sum of feature maps\n",
        "    cam = F.relu((weights * act).sum(dim=1, keepdim=True))\n",
        "\n",
        "    # upsample to input size\n",
        "    cam_up = F.interpolate(cam, size=input_tensor.shape[-1], mode='linear', align_corners=False)\n",
        "    cam_np = cam_up.squeeze().detach().cpu().numpy()\n",
        "\n",
        "    # normalize\n",
        "    cam_norm = (cam_np - cam_np.min()) / (cam_np.max() - cam_np.min() + 1e-9)\n",
        "\n",
        "    prob = float(torch.softmax(out.detach(), dim=1)[0, target_class].cpu().numpy())\n",
        "    return cam_norm, prob\n"
      ],
      "metadata": {
        "id": "N9ac7bBmKJCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======= One self-contained Grad-CAM generation cell (run after model, test_dataset, device exist) =======\n",
        "import os, random, math\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# ---------- User-editable settings ----------\n",
        "MODEL_NAME = globals().get('MODEL_NAME', 'ResNet18_1D')\n",
        "RESULTS_ROOT = globals().get('RESULTS_ROOT', '/content/drive/MyDrive/ECG_Datasets/Results')\n",
        "SAVE_DIR = Path(RESULTS_ROOT) / MODEL_NAME\n",
        "MAX_PER_CLASS = 5     # how many images per class to save\n",
        "MAX_SCAN = 5000       # how many test samples to scan (speed)\n",
        "RANDOM_SEED = 42\n",
        "# --------------------------------------------\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# Basic checks\n",
        "model = globals().get('model', None)\n",
        "test_dataset = globals().get('test_dataset', None)\n",
        "device = globals().get('device', None)\n",
        "classes = globals().get('classes', None)\n",
        "if classes is None:\n",
        "    classes = ['N','S','V','F','Q']\n",
        "\n",
        "if model is None or test_dataset is None or device is None:\n",
        "    raise RuntimeError(\"Missing required objects: ensure 'model', 'test_dataset', and 'device' are defined in the session.\")\n",
        "\n",
        "# ensure output dir\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# helper: find last Conv1d\n",
        "def find_last_conv1d(mod):\n",
        "    last = None\n",
        "    last_name = None\n",
        "    for name, m in mod.named_modules():\n",
        "        if isinstance(m, torch.nn.Conv1d):\n",
        "            last = m\n",
        "            last_name = name\n",
        "    return last, last_name\n",
        "\n",
        "# Grad-CAM function (uses forward hook only, no backward hooks that persist)\n",
        "def gradcam_1d_nohook(model, input_tensor, target_class, target_layer):\n",
        "    \"\"\"\n",
        "    input_tensor: (1,1,L) tensor on device\n",
        "    returns: cam_norm (L,) and predicted probability for target_class\n",
        "    \"\"\"\n",
        "    activations = []\n",
        "\n",
        "    def forward_hook(m, inp, out):\n",
        "        activations.append(out)   # keep tensor that requires grad\n",
        "\n",
        "    fh = target_layer.register_forward_hook(forward_hook)\n",
        "    try:\n",
        "        # forward (do NOT wrap in no_grad because we need grads w.r.t. activations)\n",
        "        out = model(input_tensor)          # (1, C)\n",
        "        score = out[0, target_class]       # scalar tensor\n",
        "\n",
        "        if len(activations) == 0:\n",
        "            raise RuntimeError(\"Activation not captured by forward hook.\")\n",
        "\n",
        "        act = activations[0]               # (1, C, Lf), requires_grad=True\n",
        "\n",
        "        # compute gradients of score w.r.t. activations\n",
        "        grads = torch.autograd.grad(outputs=score, inputs=act, retain_graph=False, create_graph=False)[0]  # (1,C,Lf)\n",
        "\n",
        "        # channel weights: global average pooling over time dim\n",
        "        weights = grads.mean(dim=2, keepdim=True)   # (1,C,1)\n",
        "\n",
        "        # weighted sum and ReLU\n",
        "        cam = F.relu((weights * act).sum(dim=1, keepdim=True))  # (1,1,Lf)\n",
        "\n",
        "        # upsample to input length\n",
        "        cam_up = F.interpolate(cam, size=input_tensor.shape[-1], mode='linear', align_corners=False)\n",
        "        cam_np = cam_up.squeeze().detach().cpu().numpy()\n",
        "\n",
        "        # normalize to [0,1]\n",
        "        cam_norm = (cam_np - cam_np.min()) / (cam_np.max() - cam_np.min() + 1e-9)\n",
        "\n",
        "        prob = float(torch.softmax(out.detach(), dim=1)[0, target_class].cpu().numpy())\n",
        "        return cam_norm, prob\n",
        "\n",
        "    finally:\n",
        "        fh.remove()\n",
        "\n",
        "# find target layer\n",
        "target_layer, layer_name = find_last_conv1d(model)\n",
        "if target_layer is None:\n",
        "    raise RuntimeError(\"No Conv1d layer found in model for Grad-CAM. Make sure the model contains Conv1d layers.\")\n",
        "print(\"Using target layer for Grad-CAM:\", layer_name)\n",
        "\n",
        "# Collect predictions/probs for test set (scan subset for speed)\n",
        "n_test = len(test_dataset)\n",
        "idxs = list(range(n_test))\n",
        "if n_test > MAX_SCAN:\n",
        "    idxs = random.sample(idxs, MAX_SCAN)\n",
        "\n",
        "softmax = torch.nn.Softmax(dim=1)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "preds = []\n",
        "probs_all = []\n",
        "labels = []\n",
        "scan_map = []  # map index in scanned list -> original dataset idx\n",
        "with torch.no_grad():\n",
        "    for i in idxs:\n",
        "        x, y = test_dataset[i]\n",
        "        inp = x.unsqueeze(0).to(device)\n",
        "        out = model(inp)\n",
        "        prob = softmax(out).cpu().numpy()[0]\n",
        "        pred = int(prob.argmax())\n",
        "        preds.append(pred)\n",
        "        probs_all.append(prob)\n",
        "        labels.append(int(y.item()))\n",
        "        scan_map.append(i)\n",
        "\n",
        "# Build per-class selection lists (prefer high-confidence correct predictions)\n",
        "per_class_indices = {c: [] for c in range(len(classes))}\n",
        "\n",
        "# first pass: correct predictions (gather tuples of (original_idx, confidence))\n",
        "for local_idx, orig_idx in enumerate(scan_map):\n",
        "    true_cls = labels[local_idx]\n",
        "    pred_cls = preds[local_idx]\n",
        "    conf = float(probs_all[local_idx][pred_cls])\n",
        "    if pred_cls == true_cls and len(per_class_indices[true_cls]) < MAX_PER_CLASS:\n",
        "        per_class_indices[true_cls].append((orig_idx, conf))\n",
        "\n",
        "# second pass: fill with any samples of that class if not enough\n",
        "for local_idx, orig_idx in enumerate(scan_map):\n",
        "    true_cls = labels[local_idx]\n",
        "    if len(per_class_indices[true_cls]) >= MAX_PER_CLASS:\n",
        "        continue\n",
        "    existing = [t[0] for t in per_class_indices[true_cls]]\n",
        "    if orig_idx in existing:\n",
        "        continue\n",
        "    # prefer samples where model predicted that class (even if wrong), else accept any\n",
        "    pred_cls = preds[local_idx]\n",
        "    conf = float(probs_all[local_idx][pred_cls])\n",
        "    per_class_indices[true_cls].append((orig_idx, conf))\n",
        "\n",
        "# trim and sort by confidence\n",
        "for cls in per_class_indices:\n",
        "    lst = per_class_indices[cls]\n",
        "    lst_sorted = sorted(lst, key=lambda x: x[1], reverse=True)[:MAX_PER_CLASS]\n",
        "    per_class_indices[cls] = [t[0] for t in lst_sorted]\n",
        "\n",
        "print(\"Collected examples per class (idx lists):\")\n",
        "for cls in range(len(classes)):\n",
        "    print(f\"  {classes[cls]}: {per_class_indices[cls]}\")\n",
        "\n",
        "# Generate and save images\n",
        "for cls_idx, cls_name in enumerate(classes):\n",
        "    chosen_idxs = per_class_indices.get(cls_idx, [])\n",
        "    if len(chosen_idxs) == 0:\n",
        "        print(f\"No examples found for class {cls_name}, skipping.\")\n",
        "        continue\n",
        "\n",
        "    # create directory for this class\n",
        "    class_dir = SAVE_DIR / f\"gradcam_{cls_name}\"\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "    # Save individual images\n",
        "    for j, idx in enumerate(chosen_idxs):\n",
        "        sample, y = test_dataset[idx]\n",
        "        x = sample.squeeze().cpu().numpy()\n",
        "        inp = sample.unsqueeze(0).to(device)\n",
        "\n",
        "        # get model prediction (with no_grad for speed)\n",
        "        with torch.no_grad():\n",
        "            out = model(inp)\n",
        "            pred = int(out.argmax(1).cpu().numpy()[0])\n",
        "\n",
        "        # compute cam (this needs autograd)\n",
        "        cam, prob = gradcam_1d_nohook(model, inp, pred, target_layer)\n",
        "\n",
        "        # plot single figure\n",
        "        fig, ax = plt.subplots(1,1, figsize=(10,2.4))\n",
        "        t = np.arange(len(x))\n",
        "        ax.plot(t, x, color='k', linewidth=0.8)\n",
        "        amp = x.max() - x.min() if x.max() != x.min() else 1.0\n",
        "        base = x.min() - 0.12*amp\n",
        "        overlay = base + cam * (0.25*amp)\n",
        "        ax.fill_between(t, base, overlay, where=cam>0, color='orange', alpha=0.6)\n",
        "        ax.scatter(t, x, c=cam, cmap='jet', s=5)\n",
        "        ax.set_xlim(0, len(x)); ax.set_yticks([])\n",
        "        ax.set_title(f\"{cls_name} — true:{classes[int(y)]} pred:{classes[pred]} p={prob:.2f}\", fontsize=10)\n",
        "\n",
        "        indiv_path = class_dir / f\"{MODEL_NAME}_gradcam_{cls_name}_{j+1}_idx{idx}.png\"\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(indiv_path, dpi=200, bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "\n",
        "    # Create a combined grid for this class (1 row, ncols = len(chosen_idxs))\n",
        "    ncols = len(chosen_idxs)\n",
        "    fig, axes = plt.subplots(1, ncols, figsize=(4*ncols, 2.6))\n",
        "    if ncols == 1:\n",
        "        axes = [axes]\n",
        "    for ax, idx in zip(axes, chosen_idxs):\n",
        "        sample, y = test_dataset[idx]\n",
        "        x = sample.squeeze().cpu().numpy()\n",
        "        inp = sample.unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model(inp)\n",
        "            pred = int(out.argmax(1).cpu().numpy()[0])\n",
        "        cam, prob = gradcam_1d_nohook(model, inp, pred, target_layer)\n",
        "\n",
        "        t = np.arange(len(x))\n",
        "        ax.plot(t, x, color='k', linewidth=0.8)\n",
        "        amp = x.max() - x.min() if x.max() != x.min() else 1.0\n",
        "        base = x.min() - 0.12*amp\n",
        "        overlay = base + cam * (0.25*amp)\n",
        "        ax.fill_between(t, base, overlay, where=cam>0, color='orange', alpha=0.6)\n",
        "        ax.scatter(t, x, c=cam, cmap='jet', s=5)\n",
        "        ax.set_xlim(0, len(x)); ax.set_yticks([])\n",
        "        ax.set_title(f\"idx:{idx} pred:{classes[pred]} p={prob:.2f}\", fontsize=9)\n",
        "\n",
        "    plt.suptitle(f\"{MODEL_NAME} Grad-CAM examples — Class {cls_name}\", fontsize=12)\n",
        "    plt.tight_layout(rect=[0,0,1,0.95])\n",
        "    combined_path = SAVE_DIR / f\"{MODEL_NAME}_gradcam_{cls_name}_grid.png\"\n",
        "    plt.savefig(combined_path, dpi=200, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "    print(f\"Saved {len(chosen_idxs)} individual images + grid for class {cls_name} in {class_dir} and {combined_path}\")\n",
        "\n",
        "print(\"Done. All images saved under:\", SAVE_DIR)\n"
      ],
      "metadata": {
        "id": "VZQM1FuoKKrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======= Minimal Grad-CAM (1–2 samples per class) =======\n",
        "import os, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------ SETTINGS ------------\n",
        "MODEL_NAME = \"ResNet18_1D\"\n",
        "RESULTS_ROOT = \"/content/drive/MyDrive/ECG_Datasets/Results\"\n",
        "SAVE_DIR = Path(RESULTS_ROOT) / MODEL_NAME\n",
        "MAX_PER_CLASS = 2     # Only 1–2 Grad-CAMs per class\n",
        "MAX_SCAN = 3000\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "# -----------------------------------\n",
        "\n",
        "model.eval()\n",
        "softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "# -------- Find last Conv1D layer --------\n",
        "def find_last_conv1d(mod):\n",
        "    last = None\n",
        "    for name, layer in mod.named_modules():\n",
        "        if isinstance(layer, torch.nn.Conv1d):\n",
        "            last = layer\n",
        "    return last\n",
        "\n",
        "target_layer = find_last_conv1d(model)\n",
        "print(\"Target Conv Layer:\", target_layer)\n",
        "\n",
        "# -------- Grad-CAM Function --------\n",
        "def gradcam_1d_nohook(model, input_tensor, target_class, target_layer):\n",
        "    activations = []\n",
        "    def forward_hook(m, inp, out):\n",
        "        activations.append(out)\n",
        "    fh = target_layer.register_forward_hook(forward_hook)\n",
        "\n",
        "    out = model(input_tensor)\n",
        "    score = out[0, target_class]\n",
        "\n",
        "    act = activations[0]\n",
        "    grads = torch.autograd.grad(score, act)[0]\n",
        "\n",
        "    fh.remove()\n",
        "\n",
        "    weights = grads.mean(dim=2, keepdim=True)\n",
        "    cam = F.relu((weights * act).sum(dim=1, keepdim=True))\n",
        "    cam_up = F.interpolate(cam, size=input_tensor.shape[-1], mode='linear')\n",
        "    cam_np = cam_up.squeeze().detach().cpu().numpy()\n",
        "    cam_norm = (cam_np - cam_np.min()) / (cam_np.max() - cam_np.min() + 1e-9)\n",
        "\n",
        "    prob = float(torch.softmax(out, dim=1)[0, target_class].cpu().numpy())\n",
        "    return cam_norm, prob\n",
        "\n",
        "# -------- Select 1–2 samples per class --------\n",
        "classes = [\"N\",\"S\",\"V\",\"F\",\"Q\"]\n",
        "per_class = {c: [] for c in range(len(classes))}\n",
        "\n",
        "idxs = list(range(min(MAX_SCAN, len(test_dataset))))\n",
        "random.shuffle(idxs)\n",
        "\n",
        "for i in idxs:\n",
        "    x, y = test_dataset[i]\n",
        "    inp = x.unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(inp)\n",
        "    pred = int(out.argmax(1))\n",
        "\n",
        "    if len(per_class[pred]) < MAX_PER_CLASS:\n",
        "        per_class[pred].append(i)\n",
        "\n",
        "    if all(len(v) >= MAX_PER_CLASS for v in per_class.values()):\n",
        "        break\n",
        "\n",
        "print(\"Selected samples per class:\", per_class)\n",
        "\n",
        "# -------- Generate Grad-CAM Plots --------\n",
        "for cls_idx, cls_name in enumerate(classes):\n",
        "    sample_idxs = per_class[cls_idx]\n",
        "    for j, idx in enumerate(sample_idxs):\n",
        "        sample, y = test_dataset[idx]\n",
        "        x = sample.squeeze().cpu().numpy()\n",
        "        inp = sample.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(inp)\n",
        "            pred = int(out.argmax(1))\n",
        "\n",
        "        cam, prob = gradcam_1d_nohook(model, inp, pred, target_layer)\n",
        "\n",
        "        # --- Plot ---\n",
        "        t = np.arange(len(x))\n",
        "        fig, ax = plt.subplots(1,1, figsize=(10,2.5))\n",
        "        ax.plot(t, x, color='black', linewidth=1)\n",
        "        base = x.min() - 0.2*(x.max()-x.min())\n",
        "        overlay = base + cam * 0.3*(x.max()-x.min())\n",
        "\n",
        "        ax.fill_between(t, base, overlay, color='orange', alpha=0.5)\n",
        "        ax.set_title(f\"{cls_name} — pred:{classes[pred]}  p={prob:.2f}   (idx:{idx})\")\n",
        "        ax.set_yticks([])\n",
        "        ax.set_xlim(0, len(x))\n",
        "\n",
        "        out_path = SAVE_DIR / f\"GradCAM_{cls_name}_{j+1}.png\"\n",
        "        plt.savefig(out_path, dpi=200, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "print(\"\\nSaved 1–2 Grad-CAM images per class in:\", SAVE_DIR)\n"
      ],
      "metadata": {
        "id": "AkdbcW62VYR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"ResNet18_1D\"\n",
        "model_dir = f\"/content/drive/MyDrive/ECG_Datasets/Results/{MODEL_NAME}/{MODEL_NAME}_best.pt\"\n",
        "\n",
        "model = ResNet18_1D(input_length=X_train.shape[1], num_classes=5)\n",
        "model.load_state_dict(torch.load(model_dir, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n"
      ],
      "metadata": {
        "id": "9RhXXXEqV7IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IN7zNySFV74N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}